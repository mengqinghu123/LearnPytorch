{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mengqinghu123/LearnPytorch/blob/main/AutoEncoder_KMeans_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9c43bbd-5d2f-45a7-9d6f-a9fc676b1e15",
      "metadata": {
        "id": "c9c43bbd-5d2f-45a7-9d6f-a9fc676b1e15"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9bf3c892-f899-40e5-a3f6-fb55bf37f53a",
      "metadata": {
        "id": "9bf3c892-f899-40e5-a3f6-fb55bf37f53a"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rs = 1234\n",
        "np.random.seed(rs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c64d4e73-6fc4-4585-b6c5-a4e2a9643b61",
      "metadata": {
        "id": "c64d4e73-6fc4-4585-b6c5-a4e2a9643b61"
      },
      "source": [
        "## Check if GPU is available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ace1d630-1d0a-4718-be08-7f0fe2205b38",
      "metadata": {
        "id": "ace1d630-1d0a-4718-be08-7f0fe2205b38"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "dbb74580-f713-4b62-ba27-452ce32d7614",
      "metadata": {
        "id": "dbb74580-f713-4b62-ba27-452ce32d7614",
        "outputId": "c9b34e28-0142-4b96-c0ec-ea02a498f756",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Should print type='cuda' if GPU is available, otherwise 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3911c685-bff4-48f4-93fa-fa5afa20ce5e",
      "metadata": {
        "id": "3911c685-bff4-48f4-93fa-fa5afa20ce5e"
      },
      "source": [
        "## Defining the Auto-Encoder Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "78f57b9f-3d54-403f-ba2f-8f3744a787a4",
      "metadata": {
        "id": "78f57b9f-3d54-403f-ba2f-8f3744a787a4"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The Encoder network.\n",
        "    A deep neural network that learns a lower-dimensional representation of the input data by mapping it into an embedding.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size: int,\n",
        "                hidden_layers: Tuple[int],\n",
        "                 dropout_rate: float=0.2,\n",
        "                 activation=nn.ReLU()\n",
        "                ):\n",
        "        super().__init__()\n",
        "\n",
        "        # First layer, the input layer\n",
        "        self.input_layer = torch.nn.Linear(input_size, hidden_layers[0])\n",
        "        self.n_layers = 0\n",
        "\n",
        "        ######################################################\n",
        "        # Usually we could specify the layers in this way:\n",
        "        # self.dense_0 = torch.nn.Linear(input_size, hidden_layers[0])\n",
        "        # self.dense_1 = torch.nn.Linear(hidden_layers[0], hidden_layers[1])\n",
        "        # ....\n",
        "        #\n",
        "        # However, instead of hardcoding this, we can do it automatically based on the hidden_layers\n",
        "        # The output of one hidden_layer will always be the input for the next hidden_layet\n",
        "        #######################################################\n",
        "        for i in range(0, len(hidden_layers) -1):\n",
        "            setattr(self, f\"dense_{i}\", torch.nn.Linear(hidden_layers[i],\n",
        "                                                        hidden_layers[i+1])\n",
        "                   )\n",
        "            self.n_layers += 1\n",
        "\n",
        "        self.activation = activation\n",
        "        self.hidden_layers = hidden_layers\n",
        "\n",
        "        # Add dropout to prevent overfitting\n",
        "        self.dropout  = nn.Dropout(dropout_rate)\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.input_size = input_size\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # Special Treatment for input layer\n",
        "        x = self.activation(self.input_layer(x))\n",
        "\n",
        "        #################################################\n",
        "        # forward pass through the dense layers\n",
        "        # We could have written each dense layer explicitly:\n",
        "        # x = self.activation(self.dense_0(x))\n",
        "        # x = self.dropout(x)\n",
        "        # x = self.activation(self.dense_1(x))\n",
        "        # .....\n",
        "        #\n",
        "        # But we do it automatically:\n",
        "        ##################################################\n",
        "        for i in range(0, self.n_layers -1):\n",
        "            x = self.activation(getattr(self, f\"dense_{i}\")(x))\n",
        "            # dropout to prevent overfitting\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        # Use layer without activation function to output embedding\n",
        "        output_layer = getattr(self, f\"dense_{self.n_layers-1}\")\n",
        "        return output_layer(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c9dedb01-4e68-425a-99b6-a486d3670a7d",
      "metadata": {
        "id": "c9dedb01-4e68-425a-99b6-a486d3670a7d"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Same as the encoder, but the layers are in reverse order.\n",
        "    So, we pass the encoder as input and use its hidden_sizes to specify the decoder network.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 encoder,\n",
        "                 activation=nn.ReLU()\n",
        "                ):\n",
        "        super().__init__()\n",
        "        self.hidden_layers = encoder.hidden_layers\n",
        "        n_layers = encoder.n_layers\n",
        "        self.hidden_layers = self.hidden_layers[::-1]\n",
        "\n",
        "        # Reversed order -> dense_0 will be the first to apply here\n",
        "        for i in range(0, n_layers):\n",
        "            setattr(self, f\"dense_{i}\", torch.nn.Linear(self.hidden_layers[i],\n",
        "                                                        self.hidden_layers[i+1])\n",
        "                   )\n",
        "        self.output_layer = torch.nn.Linear(self.hidden_layers[-1],\n",
        "                                                        encoder.input_size)\n",
        "        self.n_layers = n_layers\n",
        "        self.activation = activation\n",
        "        self.dropout  = nn.Dropout(encoder.dropout_rate)\n",
        "\n",
        "\n",
        "    def forward(self, x:Tensor) -> Tensor:\n",
        "        for i in range(0, self.n_layers):\n",
        "            dense_i = getattr(self, f\"dense_{i}\")\n",
        "            x = dense_i(x)\n",
        "            x = self.activation(x)\n",
        "            x = self.dropout(x)\n",
        "        return self.output_layer(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "03bc9e67-81b1-42a5-9aa4-181b3eb5e335",
      "metadata": {
        "id": "03bc9e67-81b1-42a5-9aa4-181b3eb5e335"
      },
      "outputs": [],
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The complete AutoEncoder that consists of the encoder and the decoder network.\n",
        "    We need this for training, but for applying the autoencoder, we will only need the encoder to map input data to an embedding.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size: int,\n",
        "                hidden_layers: Tuple[int],\n",
        "                 dropout_rate: float=0.2,\n",
        "                activation=nn.ReLU()):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(input_size, hidden_layers, dropout_rate)\n",
        "        self.decoder = Decoder(self.encoder)\n",
        "        self.hidden_layers = hidden_layers\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tuple[Tensor]:\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return encoded, decoded\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96218cd6-3757-4c7b-a5b4-bbfa306e4347",
      "metadata": {
        "id": "96218cd6-3757-4c7b-a5b4-bbfa306e4347"
      },
      "source": [
        "### Loading MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "dd5ab1a3-eb94-44e4-b941-37a4d0efe89f",
      "metadata": {
        "id": "dd5ab1a3-eb94-44e4-b941-37a4d0efe89f",
        "outputId": "99bc7795-ac75-45f9-c523-4e2154bfe837",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 37.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 18.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 37.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.29MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import ConcatDataset\n",
        "from torchvision import transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.5,), (0.5,)),\n",
        "                              ])\n",
        "trainset = MNIST('./', download=True,\n",
        "                 train=True,\n",
        "                 transform=transform)\n",
        "testset = MNIST('./', download=True,\n",
        "                 train=False,\n",
        "                 transform=transform)\n",
        "dataset = ConcatDataset([trainset, testset])\n",
        "dataloader = torch.utils.data.DataLoader(dataset,\n",
        "                                         batch_size=256,\n",
        "                                         shuffle=True,\n",
        "                                         num_workers=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e04b913a-ae78-46c0-95b3-6ed4fc451e24",
      "metadata": {
        "id": "e04b913a-ae78-46c0-95b3-6ed4fc451e24",
        "outputId": "2bafc286-88bd-47e7-db84-0557f7c6017e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "X_train = trainset.data.numpy().reshape(60000, 784)\n",
        "X_test = testset.data.numpy().reshape(10000, 784)\n",
        "X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9d54bb60-0502-404b-855e-124bb3b115ad",
      "metadata": {
        "id": "9d54bb60-0502-404b-855e-124bb3b115ad"
      },
      "outputs": [],
      "source": [
        "y_train = np.array(trainset.targets)\n",
        "y_test = np.array(testset.targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e088416a-a7f7-4c8b-abd3-c09138017a33",
      "metadata": {
        "id": "e088416a-a7f7-4c8b-abd3-c09138017a33",
        "outputId": "b744eaf9-b119-4b10-c953-79760e851307",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(70000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "y = np.concatenate([y_train, y_test])\n",
        "X = np.concatenate([X_train, X_test])\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c62a55a-f9b3-4c75-8038-203fa1647546",
      "metadata": {
        "id": "7c62a55a-f9b3-4c75-8038-203fa1647546"
      },
      "source": [
        "### Create Model and Specify Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a8bc910e-8ce7-4b23-9b6e-b01e6b4b6cbf",
      "metadata": {
        "id": "a8bc910e-8ce7-4b23-9b6e-b01e6b4b6cbf"
      },
      "outputs": [],
      "source": [
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "loss_ = nn.MSELoss()\n",
        "n_input_features = X.shape[1]\n",
        "# Initialize architecture of our Auto-Encoder\n",
        "model = AutoEncoder(input_size=n_input_features,\n",
        "                    hidden_layers=[500, 500, 2000,\n",
        "                                 10 # This is the dimension of the embedding\n",
        "                                 ],\n",
        "                   # Prevent overfitting by deactivating 20% of the neurons during training\n",
        "                    dropout_rate=0.2\n",
        "                   ).to(device) # use GPU if available\n",
        "\n",
        "# Activate training mode\n",
        "model.train()\n",
        "\n",
        "# We could restore a model to continue training from a checkpoint\n",
        "#model = torch.load(\"./torch_models/autoencoder\")\n",
        "\n",
        "# Learning Rate\n",
        "lr = 0.1\n",
        "\n",
        "# Use Stochastic Gradient Descent as optimizer with momentum 0.9\n",
        "optimizer = torch.optim.SGD(lr=lr,\n",
        "                            momentum=0.9,\n",
        "                            params=model.parameters())\n",
        "\n",
        "# reduce learning rate as training continues\n",
        "scheduler = lr_scheduler.StepLR(optimizer,\n",
        "                                  step_size=100,\n",
        "                                  gamma=0.1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5f527de-bdb3-4524-a66f-3837ee03ed42",
      "metadata": {
        "id": "c5f527de-bdb3-4524-a66f-3837ee03ed42"
      },
      "source": [
        "### Pre-train AutoEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ce9d1e3-ffb1-4cf3-95dc-79a60586a70a",
      "metadata": {
        "scrolled": true,
        "id": "6ce9d1e3-ffb1-4cf3-95dc-79a60586a70a"
      },
      "outputs": [],
      "source": [
        "n_epochs = 300\n",
        "eval_every = 10\n",
        "best_loss = np.infty\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    losses = []\n",
        "    # Iterate over data in batches\n",
        "    for x_batch, y_batch in dataloader:\n",
        "        # PyTorch specific; We need to reset all gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 0. Transform input batch data from 28 X 28 to 784 features\n",
        "        #   Note that our encoder maps the data into just 10 features!\n",
        "        x_batch = x_batch.to(device)\n",
        "        x_batch = x_batch.view(x_batch.shape[0], -1)\n",
        "\n",
        "        # 1. Apply AutoEncoder model (forward pass).\n",
        "        #    We use the output of the decoder for training.\n",
        "        output = model(x_batch)[1]\n",
        "\n",
        "        # 2. Calculate the reconstruction loss\n",
        "        loss = loss_(output, x_batch)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # 3. Backpropagate less\n",
        "        loss.backward()\n",
        "\n",
        "        # 4. Update the weights\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "    mean_loss = np.round(np.mean(losses),\n",
        "                         5)\n",
        "    if (epoch+1) % eval_every == 0:\n",
        "        print(f\"Loss at epoch [{epoch+1} / {n_epochs}]: {mean_loss}\")\n",
        "\n",
        "    # Update learning rate as training continues\n",
        "    scheduler.step()\n",
        "\n",
        "    if mean_loss < best_loss:\n",
        "        best_loss = loss\n",
        "        # Store the model\n",
        "        torch.save(model, \"./torch_models/autoencoder\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfc91791-940b-4069-a2db-589e96e05e40",
      "metadata": {
        "id": "cfc91791-940b-4069-a2db-589e96e05e40"
      },
      "source": [
        "#### Fine-Tune Auto-Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97ef42c6-2d99-4f61-9479-f19fd8a121d9",
      "metadata": {
        "id": "97ef42c6-2d99-4f61-9479-f19fd8a121d9",
        "outputId": "95b5fadd-fc7c-48db-a303-347e8a6a43b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss at epoch [10 / 100]: 0.04415\n",
            "Loss at epoch [20 / 100]: 0.0443\n",
            "Loss at epoch [30 / 100]: 0.04442\n",
            "Loss at epoch [40 / 100]: 0.0445\n",
            "Loss at epoch [50 / 100]: 0.04457\n",
            "Loss at epoch [60 / 100]: 0.04461\n",
            "Loss at epoch [70 / 100]: 0.04464\n",
            "Loss at epoch [80 / 100]: 0.04466\n",
            "Loss at epoch [90 / 100]: 0.04466\n",
            "Loss at epoch [100 / 100]: 0.04466\n"
          ]
        }
      ],
      "source": [
        "# Load the model\n",
        "model = torch.load(\"./torch_models/autoencoder\")\n",
        "\n",
        "# Inference Mode for fine-tuning\n",
        "model.eval()\n",
        "\n",
        "lr = 0.1\n",
        "optimizer = torch.optim.SGD(lr=lr,\n",
        "                            momentum=0.9,\n",
        "                            params=model.parameters()\n",
        "                           )\n",
        "n_epochs = 100\n",
        "eval_every = 10\n",
        "best_loss = np.infty\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for x_batch, y_batch in dataloader:\n",
        "        # Reset gradients --> Specific for PyTorch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Use GPU\n",
        "        x_batch = x_batch.to(device)\n",
        "\n",
        "        # Image has shape 28 x 28 -> Transform to 784 features using flattening\n",
        "        x_batch = x_batch.view(x_batch.shape[0], -1)\n",
        "\n",
        "        # Apply the model\n",
        "        output = model(x_batch)[1]\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = loss_(output, x_batch)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # Backpropagate the loss\n",
        "        loss.backward()\n",
        "\n",
        "        # update weights\n",
        "        optimizer.step()\n",
        "\n",
        "    mean_loss = np.round(np.mean(losses),5)\n",
        "    if (epoch+1) % eval_every == 0:\n",
        "        print(f\"Loss at epoch [{epoch+1} / {n_epochs}]: {mean_loss}\")\n",
        "    torch.save(model, \"./torch_models/autoencoder-finetuned\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f9ebe3a-a2e1-4061-83f6-2395d3c0ce25",
      "metadata": {
        "id": "5f9ebe3a-a2e1-4061-83f6-2395d3c0ce25"
      },
      "source": [
        "## Baseline KMeans Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8a01566-2f34-4f60-a6d9-d8dd845dd3f3",
      "metadata": {
        "id": "c8a01566-2f34-4f60-a6d9-d8dd845dd3f3"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "# Use the actual number of clusters as parameter\n",
        "n_clusters = len(np.unique(y))\n",
        "\n",
        "# Apply kmeans using sklearn\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=rs)\n",
        "\n",
        "# Get training predictions\n",
        "y_pred_kmeans = kmeans.fit_predict(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2494d11c-0d06-4e7a-b4ab-4ef38f7180de",
      "metadata": {
        "id": "2494d11c-0d06-4e7a-b4ab-4ef38f7180de",
        "outputId": "d75e7fbc-ba24-4c32-9239-bb83d6b02c6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of k-Means Clustering:\n",
            "AMI: 0.5\n",
            "ARI: 0.367\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import adjusted_mutual_info_score, adjusted_rand_score\n",
        "print(\"Accuracy of k-Means Clustering:\")\n",
        "ami_kmeans = adjusted_mutual_info_score(y, y_pred_kmeans)\n",
        "ari_kmeans = adjusted_rand_score(y, y_pred_kmeans)\n",
        "print(f\"AMI: {np.round(ami_kmeans, 3)}\")\n",
        "print(f\"ARI: {np.round(ari_kmeans, 3)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acccda24-a6c5-4566-9e4e-7c50a5db5d53",
      "metadata": {
        "id": "acccda24-a6c5-4566-9e4e-7c50a5db5d53"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ece0847a-5cb2-496c-a582-f8e691baa724",
      "metadata": {
        "id": "ece0847a-5cb2-496c-a582-f8e691baa724"
      },
      "source": [
        "## Apply Auto-Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5187531-7d26-4913-8d6b-ed89116c1deb",
      "metadata": {
        "id": "f5187531-7d26-4913-8d6b-ed89116c1deb"
      },
      "source": [
        "### Evaluate Pre-trained Auto-Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f66a0d65-513c-43e8-811f-4eb488c3b782",
      "metadata": {
        "id": "f66a0d65-513c-43e8-811f-4eb488c3b782"
      },
      "outputs": [],
      "source": [
        "model = torch.load(\"./torch_models/autoencoder\")\n",
        "X_embedded_pretrained = model(Tensor(X).to(device))[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa1a7f4d-bc77-4724-8585-1da86f5af4da",
      "metadata": {
        "id": "fa1a7f4d-bc77-4724-8585-1da86f5af4da"
      },
      "outputs": [],
      "source": [
        "# Apply kmeans using sklearn\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=rs)\n",
        "\n",
        "# Convert Data to CPU and apply kmeans to get the cluster predictions\n",
        "y_pred_AE_pretrained = kmeans.fit_predict(X_embedded_pretrained.detach().cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a483888-3997-4ed1-a99c-4dc34953796a",
      "metadata": {
        "id": "6a483888-3997-4ed1-a99c-4dc34953796a",
        "outputId": "062024fa-fbb6-41ab-9833-9a1c5937c0ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for Auto-Encoder:\n",
            "AMI: 55.4\n",
            "ARI: 47.1\n"
          ]
        }
      ],
      "source": [
        "print(\"Accuracy for Auto-Encoder:\")\n",
        "ami_AE_pretrained = adjusted_mutual_info_score(y, y_pred_AE_pretrained)\n",
        "ari_AE_pretrained = adjusted_rand_score(y, y_pred_AE_pretrained)\n",
        "print(f\"AMI: {np.round(ami_AE_pretrained * 100, 1)}\")\n",
        "print(f\"ARI: {np.round(ari_AE_pretrained * 100, 1)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75e14f2b-31e8-4726-8d94-554664f60911",
      "metadata": {
        "id": "75e14f2b-31e8-4726-8d94-554664f60911"
      },
      "source": [
        "### Evaluate Fine-tuned Auto-Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de956cdd-6e91-42ca-b327-8f1aa1f03d5d",
      "metadata": {
        "id": "de956cdd-6e91-42ca-b327-8f1aa1f03d5d"
      },
      "outputs": [],
      "source": [
        "model = torch.load(\"./torch_models/autoencoder-finetuned-old\")\n",
        "X_embedded = model(Tensor(X).to(device))[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7f4a62e-ebd8-4048-a5e7-068d795f6b5d",
      "metadata": {
        "id": "c7f4a62e-ebd8-4048-a5e7-068d795f6b5d"
      },
      "outputs": [],
      "source": [
        "# Apply kmeans using sklearn\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=rs)\n",
        "\n",
        "# Get training predictions\n",
        "y_pred_AE_finetuned = kmeans.fit_predict(X_embedded.detach().cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0397e545-0621-4cad-b2fc-3698026cc48c",
      "metadata": {
        "id": "0397e545-0621-4cad-b2fc-3698026cc48c",
        "outputId": "0af4ed7e-36e6-4594-dba1-6c055aecf74d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for Auto-Encoder:\n",
            "AMI: 72.8\n",
            "ARI: 66.3\n"
          ]
        }
      ],
      "source": [
        "print(\"Accuracy for Auto-Encoder:\")\n",
        "ami_AE_finetuned = adjusted_mutual_info_score(y, y_pred_AE_finetuned)\n",
        "ari_AE_finetuned = adjusted_rand_score(y, y_pred_AE_finetuned)\n",
        "print(f\"AMI: {np.round(ami_AE_finetuned*100, 1)}\")\n",
        "print(f\"ARI: {np.round(ari_AE_finetuned*100, 1)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99c56f04-1748-4a8e-aafe-fb2cf34568e8",
      "metadata": {
        "id": "99c56f04-1748-4a8e-aafe-fb2cf34568e8"
      },
      "source": [
        "## Overall Evaluation Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17e8a1e0-6b7c-4854-b673-3439ff485338",
      "metadata": {
        "id": "17e8a1e0-6b7c-4854-b673-3439ff485338"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame({\"Clustering Approach\": [\"k-Means\", \"Auto-Encoder (pre-trained)\", \"Auto-Encoder (fine-tuned)\"],\n",
        "                   \"AMI\": [ami_kmeans, ami_AE_pretrained, ami_AE_finetuned],\n",
        "                  \"ARI\": [ari_kmeans, ari_AE_pretrained, ari_AE_finetuned]})\n",
        "df[\"AMI\"] *= 100\n",
        "df[\"ARI\"] *= 100\n",
        "df[\"AMI\"] = df[\"AMI\"].round(1)\n",
        "df[\"ARI\"] = df[\"ARI\"].round(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79f3119c-7579-45d4-b3e2-8bcf21d55c0f",
      "metadata": {
        "id": "79f3119c-7579-45d4-b3e2-8bcf21d55c0f",
        "outputId": "a70bd7a4-389d-47d6-af80-d22ab5be20f0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Clustering Approach</th>\n",
              "      <th>AMI</th>\n",
              "      <th>ARI</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>k-Means</td>\n",
              "      <td>50.0</td>\n",
              "      <td>36.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Auto-Encoder (pre-trained)</td>\n",
              "      <td>55.4</td>\n",
              "      <td>47.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Auto-Encoder (fine-tuned)</td>\n",
              "      <td>72.8</td>\n",
              "      <td>66.3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Clustering Approach   AMI   ARI\n",
              "0                     k-Means  50.0  36.7\n",
              "1  Auto-Encoder (pre-trained)  55.4  47.1\n",
              "2   Auto-Encoder (fine-tuned)  72.8  66.3"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}